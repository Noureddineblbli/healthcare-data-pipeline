version: '3.8'

services:
  postgres:
    image: postgres:13-alpine
    container_name: airflow-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT}:5432"
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "${POSTGRES_USER}", "-d", "${POSTGRES_DB}" ]
      interval: 5s
      timeout: 5s
      retries: 5

  airflow-init:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
    command: >
      bash -c "
        airflow db upgrade;
        airflow users create --username ${AIRFLOW_USER_NAME} --firstname admin --lastname admin --role Admin --email ${AIRFLOW_USER_EMAIL} --password ${AIRFLOW_USER_PASSWORD};
      "

  airflow-webserver:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: airflow-webserver
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      # --- ADD ALL OF THESE NEW ENVIRONMENT VARIABLES ---
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_ROLE=${SNOWFLAKE_ROLE}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/data:/opt/bitnami/spark/data
      - ./spark/jars:/opt/bitnami/spark/extra-jars
    ports:
      - "${AIRFLOW_UI_PORT}:8080"
    command: >
      bash -c "airflow webserver"
    healthcheck:
      test: [ "CMD-SHELL", "airflow db check || exit 1" ]
      interval: 30s
      timeout: 30s
      retries: 20

  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: airflow-scheduler
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      # --- ADD ALL OF THESE NEW ENVIRONMENT VARIABLES ---
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_ROLE=${SNOWFLAKE_ROLE}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/data:/opt/bitnami/spark/data
      - ./spark/jars:/opt/bitnami/spark/extra-jars
    command: >
      bash -c "airflow scheduler"

  nifi:
    image: apache/nifi:1.25.0
    container_name: nifi-server
    restart: unless-stopped
    ports:
      - "${NIFI_UI_PORT}:${NIFI_WEB_HTTP_PORT}"
    environment:
      - NIFI_WEB_HTTP_PORT=${NIFI_WEB_HTTP_PORT}
    volumes:
      - nifi-conf:/opt/nifi/nifi-current/conf
      - nifi-data:/opt/nifi/nifi-current/database_repository
      - nifi-flowfile:/opt/nifi/nifi-current/flowfile_repository
      - nifi-content:/opt/nifi/nifi-current/content_repository
      - nifi-provenance:/opt/nifi/nifi-current/provenance_repository
      - ./sample_data:/opt/nifi/nifi-current/data/input
      - ./spark/data/input/fhir_json:/opt/nifi/nifi-current/data/output

  spark-master:
    build:
      # <--- MODIFIED
      context: .
      dockerfile: docker/Spark.Dockerfile
    container_name: spark-master
    restart: unless-stopped
    user: "50000"
    # The 'user: root' line is no longer needed here, it's in the Dockerfile.
    environment:
      - SPARK_MODE=${SPARK_MODE}
      - SPARK_RPC_AUTHENTICATION_ENABLED=${SPARK_RPC_AUTHENTICATION_ENABLED}
      - SPARK_RPC_ENCRYPTION_ENABLED=${SPARK_RPC_ENCRYPTION_ENABLED}
      - SPARK_CLASSPATH=/opt/bitnami/spark/extra-jars/*

      # --- ADD ALL OF THESE NEW ENVIRONMENT VARIABLES ---
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_ROLE=${SNOWFLAKE_ROLE}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
    ports:
      - "${SPARK_MASTER_UI_PORT}:8080"
      - "7077:7077"
    volumes:
      - ./spark/jobs:/opt/bitnami/spark/jobs
      - ./spark/data:/opt/bitnami/spark/data
      - ./spark/jars:/opt/bitnami/spark/extra-jars

  spark-worker:
    build:
      # <--- MODIFIED
      context: .
      dockerfile: docker/Spark.Dockerfile
    container_name: spark-worker
    restart: unless-stopped
    depends_on:
      - spark-master
    user: "50000"
    # The 'user: root' line is no longer needed here, it's in the Dockerfile.
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./spark/jobs:/opt/bitnami/spark/jobs
      - ./spark/data:/opt/bitnami/spark/data
      - ./spark/jars:/opt/bitnami/spark/extra-jars

volumes:
  postgres-data:
  nifi-conf:
  nifi-data:
  nifi-flowfile:
  nifi-content:
  nifi-provenance:
